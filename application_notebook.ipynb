{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our dependencies\n",
    "from pymongo import MongoClient\n",
    "from pprint import pprint\n",
    "from tabulate import tabulate\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to MongoDB Atlas\n",
    "client = MongoClient(\"mongodb+srv://group_user:UTProj4@project4.ofsuk.mongodb.net/remote?retryWrites=true&w=majority\")\n",
    "\n",
    "# Access the 'remote' database\n",
    "db = client['remote']\n",
    "\n",
    "# Fetch data from the 'record_df_df_dfs' collection\n",
    "data_cursor = db['records'].find()\n",
    "data_list = list(data_cursor)\n",
    "data_df = pd.DataFrame(data_list)  # Convert to DataFrame\n",
    "\n",
    "# Fetch data from the 'credit' collection\n",
    "record_cursor = db['credit'].find()\n",
    "record_list = list(record_cursor)\n",
    "record_df = pd.DataFrame(record_list)  # Convert to DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group and process the 'credit' collection DataFrame\n",
    "begin_month = record_df.groupby(\"ID\")[\"MONTHS_BALANCE\"].agg(min).reset_index()\n",
    "begin_month = begin_month.rename(columns={'MONTHS_BALANCE': 'begin_month'})\n",
    "\n",
    "# Merge the DataFrames on the 'ID' column\n",
    "merged_df = pd.merge(data_df, begin_month, how=\"left\", on=\"ID\")\n",
    "merged_df = pd.DataFrame(merged_df)\n",
    "\n",
    "# Show the first few rows of the merged DataFrame\n",
    "print(merged_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all users' account open month\n",
    "begin_month = record_df.groupby(\"ID\", as_index=False)[\"MONTHS_BALANCE\"].agg(\"min\")\n",
    "begin_month = begin_month.rename(columns={\"MONTHS_BALANCE\": \"begin_month\"})\n",
    "\n",
    "# Merge with the main dataset\n",
    "new_data = pd.merge(data_df, begin_month, how=\"left\", on=\"ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column to identify past due payments\n",
    "record_df['past_due'] = 'No'  # Default to 'No'\n",
    "record_df.loc[record_df['STATUS'].isin([2, 3, 4, 5]), 'past_due'] = 'Yes'\n",
    "\n",
    "# Group by 'ID' to determine if any record is past due\n",
    "risk_factor = record_df.groupby('ID', as_index=False)['past_due'].agg(\n",
    "    lambda x: 'Yes' if 'Yes' in x.values else 'No'\n",
    ")\n",
    "\n",
    "# Debug: Ensure 'cpunt' contains 'past_due'\n",
    "print(\"risk_factor columns:\", risk_factor.columns)\n",
    "print(risk_factor.head())\n",
    "\n",
    "# Merge with new_data\n",
    "new_data = pd.merge(new_data, risk_factor, how='inner', on='ID')\n",
    "\n",
    "# Debug: Ensure 'past_due' exists in new_data\n",
    "print(\"new_data columns:\", new_data.columns)\n",
    "print(new_data.head())\n",
    "\n",
    "# Map 'Yes' and 'No' to numeric targets\n",
    "if 'past_due' in new_data.columns:\n",
    "    new_data['target'] = new_data['past_due'].map({'Yes': 1, 'No': 0})\n",
    "else:\n",
    "    print(\"Error: 'past_due' column is missing in new_data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(risk_factor['past_due'].value_counts())\n",
    "risk_factor['past_due'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(record_df['past_due'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a binary target variable (1 for high-risk customers, 0 otherwise)\n",
    "print(new_data['target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_factor = record_df.groupby('ID', as_index=False)['past_due'].agg(\n",
    "    lambda x: 'Yes' if 'Yes' in x.values else 'No'\n",
    ")\n",
    "print(new_data['ID'].dtype)\n",
    "print(risk_factor['ID'].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for missing values in 'past_due'\n",
    "na_count = new_data['past_due'].isna().sum()\n",
    "print(f\"Number of missing values in 'past_due': {na_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yes_count = (risk_factor['past_due'] == 'Yes').sum()\n",
    "no_count = (risk_factor['past_due'] == 'No').sum()\n",
    "\n",
    "print(f\"Yes: {yes_count}\")\n",
    "print(f\"No: {no_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = new_data.rename(columns={\n",
    "    \"ID\": \"ID\",\n",
    "    \"CODE_GENDER\": \"GENDER\",\n",
    "    \"FLAG_OWN_CAR\": \"OWN CAR\",\n",
    "    \"FLAG_OWN_REALTY\": \"OWN REALTY\",\n",
    "    \"CNT_CHILDREN\": \"CHILDREN\",\n",
    "    \"AMT_INCOME_TOTAL\": \"INCOME TOTAL\",\n",
    "    \"NAME_INCOME_TYPE\": \"INCOME TYPE\",\n",
    "    \"NAME_EDUCATION_TYPE\": \"EDUCATION TYPE\",\n",
    "    \"NAME_FAMILY_STATUS\": \"FAMILY STATUS\",\n",
    "    \"NAME_HOUSING_TYPE\": \"HOUSING TYPE\",\n",
    "    \"DAYS_BIRTH\": \"DAYS SINCE BIRTH\",\n",
    "    \"DAYS_EMPLOYED\": \"DAYS EMPLOYED\",\n",
    "    \"FLAG_MOBIL\": \"MOBIL\",\n",
    "    \"FLAG_WORK_PHONE\": \"WORK PHONE\",\n",
    "    \"FLAG_PHONE\": \"PHONE\",\n",
    "    \"FLAG_EMAIL\": \"EMAIL\",\n",
    "    \"OCCUPATION_TYPE\": \"TYPE\",\n",
    "    \"CNT_FAM_MEMBERS\": \"FAM MEMBERS\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.dropna()\n",
    "new_data = new_data.mask(new_data == 'NULL').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with column names and initialize 'IV' with None\n",
    "ivtable = pd.DataFrame({'variable': new_data.columns, 'IV': None})\n",
    "\n",
    "# List of variables to exclude\n",
    "namelist = ['MOBIL', 'begin_month', 'past_due', 'target', 'ID']\n",
    "\n",
    "# Drop rows where the 'variable' is in the namelist\n",
    "ivtable = ivtable[~ivtable['variable'].isin(namelist)].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function calculates the Information Value (IV) of a feature relative to a target variable. Information Value is a measure used in predictive modeling to evaluate the predictive power of a feature. It is commonly used in credit scoring and binary classification problems.\n",
    "\n",
    "Function Workflow:\n",
    "Input Parameters:\n",
    "\n",
    "df: The DataFrame containing the feature and target variable.<br>\n",
    "feature: The column name of the feature for which IV is being calculated.<br>\n",
    "target: The column name of the target variable (binary: 0 or 1).<br>\n",
    "pr: A boolean flag to print intermediate data and the IV score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated Function\n",
    "def calc_iv(df, feature, target, pr=False):\n",
    "    # Handle missing values in a way compatible with categorical data\n",
    "    if df[feature].dtype.name == \"category\":\n",
    "        # Add \"NULL\" as a category if not already present\n",
    "        if \"NULL\" not in df[feature].cat.categories:\n",
    "            df[feature] = df[feature].cat.add_categories(\"NULL\")\n",
    "    # Replace missing values with \"NULL\"\n",
    "    df[feature] = df[feature].fillna(\"NULL\")\n",
    "\n",
    "    lst = []\n",
    "\n",
    "    # Iterate over unique feature values\n",
    "    for val in df[feature].unique():\n",
    "        all_count = df[df[feature] == val].shape[0]\n",
    "        good_count = df[(df[feature] == val) & (df[target] == 0)].shape[0]  # Good (e.g., target == 0)\n",
    "        bad_count = df[(df[feature] == val) & (df[target] == 1)].shape[0]   # Bad (e.g., target == 1)\n",
    "\n",
    "        lst.append([feature, val, all_count, good_count, bad_count])\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    data = pd.DataFrame(lst, columns=['Variable', 'Value', 'All', 'Good', 'Bad'])\n",
    "\n",
    "    # Calculate shares and distributions\n",
    "    total_good = data['Good'].sum()\n",
    "    total_bad = data['Bad'].sum()\n",
    "    epsilon = 1e-10  # Small constant to prevent division by zero\n",
    "\n",
    "    data['Share'] = data['All'] / data['All'].sum()\n",
    "    data['Bad Rate'] = data['Bad'] / (data['All'] + epsilon)\n",
    "    data['Distribution Good'] = data['Good'] / (total_good + epsilon)\n",
    "    data['Distribution Bad'] = data['Bad'] / (total_bad + epsilon)\n",
    "\n",
    "    # Calculate Weight of Evidence (WoE)\n",
    "    data['WoE'] = np.log((data['Distribution Good'] + epsilon) / (data['Distribution Bad'] + epsilon))\n",
    "    data.replace({'WoE': {np.inf: 0, -np.inf: 0}}, inplace=True)  # Replace infinite values with 0\n",
    "\n",
    "    # Calculate Information Value (IV)\n",
    "    data['IV'] = (data['Distribution Good'] - data['Distribution Bad']) * data['WoE']\n",
    "\n",
    "    # Sort data\n",
    "    data = data.sort_values(by=['Variable', 'Value'], ascending=[True, True]).reset_index(drop=True)\n",
    "\n",
    "    # Print results if needed\n",
    "    if pr:\n",
    "        print(data)\n",
    "        print('IV = ', data['IV'].sum())\n",
    "\n",
    "    # Calculate total IV\n",
    "    iv = data['IV'].sum()\n",
    "    print('This variable\\'s IV is:', iv)\n",
    "    print(df[feature].value_counts())\n",
    "\n",
    "    return iv, data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yes_count = (risk_factor['past_due'] == 'Yes').sum()\n",
    "no_count = (risk_factor['past_due'] == 'No').sum()\n",
    "\n",
    "print(f\"Yes: {yes_count}\")\n",
    "print(f\"No: {no_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_data['target'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_data.info())\n",
    "print(new_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iv, data = calc_iv(df=new_data, feature='GENDER', target='target', pr=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data['GENDER'] = new_data['GENDER'].replace(['F','M'],[0,1])\n",
    "print(new_data['GENDER'].value_counts())\n",
    "iv, data = calc_iv(new_data,'GENDER','target')\n",
    "ivtable.loc[ivtable['variable']=='GENDER','IV']=iv\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dummy(df, feature,rank=0):\n",
    "    pos = pd.get_dummies(df[feature], prefix=feature)\n",
    "    mode = df[feature].value_counts().index[rank]\n",
    "    biggest = feature + '_' + str(mode)\n",
    "    pos.drop([biggest],axis=1,inplace=True)\n",
    "    df.drop([feature],axis=1,inplace=True)\n",
    "    df=df.join(pos)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_category(df, col, binsnum, labels, qcut = False):\n",
    "    if qcut:\n",
    "        localdf = pd.qcut(df[col], q = binsnum, labels = labels) # quantile cut\n",
    "    else:\n",
    "        localdf = pd.cut(df[col], bins = binsnum, labels = labels) # equal-length cut\n",
    "        \n",
    "    localdf = pd.DataFrame(localdf)\n",
    "    name = 'gp' + '_' + col\n",
    "    localdf[name] = localdf[col]\n",
    "    df = df.join(localdf[name])\n",
    "    df[name] = df[name].astype(object)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        \n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data['OWN CAR'] = new_data['OWN CAR'].replace(['N','Y'],[0,1])\n",
    "print(new_data['OWN CAR'].value_counts())\n",
    "iv, data=calc_iv(new_data,'OWN CAR','target')\n",
    "ivtable.loc[ivtable['variable']=='OWN CAR','IV']=iv\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data['OWN REALTY'] = new_data['OWN REALTY'].replace(['N','Y'],[0,1])\n",
    "print(new_data['OWN REALTY'].value_counts())\n",
    "iv, data=calc_iv(new_data,'OWN REALTY','target')\n",
    "ivtable.loc[ivtable['variable']=='OWN REALTY','IV']=iv\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data['PHONE']=new_data['PHONE'].astype(str)\n",
    "print(new_data['PHONE'].value_counts(normalize=True,sort=False))\n",
    "new_data.drop(new_data[new_data['PHONE'] == 'nan' ].index, inplace=True)\n",
    "iv, data=calc_iv(new_data,'PHONE','target')\n",
    "ivtable.loc[ivtable['variable']=='PHONE','IV']=iv\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_data['EMAIL'].value_counts(normalize=True,sort=False))\n",
    "new_data['EMAIL']=new_data['EMAIL'].astype(str)\n",
    "iv, data=calc_iv(new_data,'EMAIL','target')\n",
    "ivtable.loc[ivtable['variable']=='EMAIL','IV']=iv\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data['WORK PHONE']=new_data['WORK PHONE'].astype(str)\n",
    "iv, data = calc_iv(new_data,'WORK PHONE','target')\n",
    "new_data.drop(new_data[new_data['WORK PHONE'] == 'nan' ].index, inplace=True)\n",
    "ivtable.loc[ivtable['variable']=='WORK PHONE','IV']=iv\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.loc[new_data['CHILDREN'] >= 2,'CHILDREN']='2More'\n",
    "print(new_data['CHILDREN'].value_counts(sort=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iv, data=calc_iv(new_data,'CHILDREN','target')\n",
    "ivtable.loc[ivtable['variable']=='CHILDREN','IV']=iv\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = convert_dummy(new_data,'CHILDREN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data['INCOME TOTAL']=new_data['INCOME TOTAL'].astype(object)\n",
    "new_data['INCOME TOTAL'] = new_data['INCOME TOTAL']/10000 \n",
    "print(new_data['INCOME TOTAL'].value_counts(bins=10,sort=False))\n",
    "new_data['INCOME TOTAL'].plot(kind='hist',bins=50,density=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create categories using pd.qcut\n",
    "new_data['gp_inc'] = pd.qcut(\n",
    "    new_data['INCOME TOTAL'], \n",
    "    q=3,  # Number of quantiles\n",
    "    labels=[\"low\", \"medium\", \"high\"]\n",
    ")\n",
    "\n",
    "# Calculate IV\n",
    "iv, data = calc_iv(new_data, 'gp_inc', 'target')\n",
    "\n",
    "# Update IV table\n",
    "ivtable.loc[ivtable['variable'] == 'INCOME TOTAL', 'IV'] = iv\n",
    "\n",
    "# Print results\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'INCOME TOTAL' to numeric and normalize\n",
    "new_data['INCOME TOTAL'] = new_data['INCOME TOTAL'].astype(float) / 10000\n",
    "\n",
    "# Print distribution\n",
    "print(new_data['INCOME TOTAL'].value_counts(bins=10, sort=False))\n",
    "\n",
    "# Plot histogram\n",
    "new_data['INCOME TOTAL'].plot(kind='hist', bins=50, density=True)\n",
    "\n",
    "# Create categories using pd.qcut\n",
    "new_data['gp_inc'] = pd.qcut(\n",
    "    new_data['INCOME TOTAL'], \n",
    "    q=3,  # Quantiles\n",
    "    labels=[\"low\", \"medium\", \"high\"]\n",
    ")\n",
    "\n",
    "# Add 'NULL' as a category to avoid errors when filling NaNs\n",
    "new_data['gp_inc'] = new_data['gp_inc'].cat.add_categories(\"NULL\").fillna(\"NULL\")\n",
    "\n",
    "# Calculate IV\n",
    "iv, data = calc_iv(new_data, 'gp_inc', 'target')\n",
    "\n",
    "# Update IV table\n",
    "ivtable.loc[ivtable['variable'] == 'INCOME TOTAL', 'IV'] = iv\n",
    "\n",
    "# Print results\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = convert_dummy(new_data,'gp_inc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Age\n",
    "new_data['Age'] = -(new_data['DAYS SINCE BIRTH']) // 365\n",
    "\n",
    "# Check distribution of Age\n",
    "print(new_data['Age'].value_counts(bins=10, normalize=True, sort=False))\n",
    "new_data['Age'].plot(kind='hist', bins=20, density=True)\n",
    "\n",
    "# Step 1: Create age categories\n",
    "new_data['gp_Age'] = pd.qcut(\n",
    "    new_data['Age'],\n",
    "    q=5,  # Number of quantiles\n",
    "    labels=[\"lowest\", \"low\", \"medium\", \"high\", \"highest\"]\n",
    ")\n",
    "\n",
    "# Step 2: Calculate IV for 'gp_Age'\n",
    "iv, data = calc_iv(new_data, 'gp_Age', 'target')\n",
    "print(\"IV Data for gp_Age:\")\n",
    "print(data[['Value', 'Good', 'Bad', 'WoE', 'IV']])\n",
    "\n",
    "# Step 3: Update the IV table for 'DAYS SINCE BIRTH'\n",
    "if 'DAYS SINCE BIRTH' not in ivtable['variable'].values:\n",
    "    # Append a new row for DAYS SINCE BIRTH if it does not exist\n",
    "    new_row = pd.DataFrame({'variable': ['DAYS SINCE BIRTH'], 'IV': [0]})\n",
    "    ivtable = pd.concat([ivtable, new_row], ignore_index=True)\n",
    "\n",
    "# Update the IV value for DAYS SINCE BIRTH\n",
    "ivtable.loc[ivtable['variable'] == 'DAYS SINCE BIRTH', 'IV'] = iv\n",
    "\n",
    "# Step 4: Convert 'gp_Age' to dummy variables\n",
    "new_data = convert_dummy(new_data, 'gp_Age')\n",
    "\n",
    "# Step 5: Sort and display the IV table\n",
    "ivtable = ivtable.sort_values(by='IV', ascending=False)\n",
    "print(ivtable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Family Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create work experience categories for 'DAYS_EMPLOYED'\n",
    "new_data['worktm'] = -(new_data['DAYS EMPLOYED']) // 365\n",
    "new_data.loc[new_data['worktm'] < 0, 'worktm'] = np.nan\n",
    "new_data['worktm'] = new_data['worktm'].fillna(new_data['worktm'].mean())\n",
    "new_data = get_category(new_data, 'worktm', 5, [\"lowest\", \"low\", \"medium\", \"high\", \"highest\"])\n",
    "\n",
    "# Step 2: Calculate IV for gp_worktm\n",
    "iv, data = calc_iv(new_data, 'gp_worktm', 'target')\n",
    "print(\"IV Data for gp_worktm:\")\n",
    "print(data[['Value', 'Good', 'Bad', 'WoE', 'IV']])\n",
    "\n",
    "# Step 3: Update the IV table for DAYS_EMPLOYED\n",
    "if 'DAYS EMPLOYED' not in ivtable['variable'].values:\n",
    "    new_row = pd.DataFrame({'variable': ['DAYS EMPLOYED'], 'IV': [0]})\n",
    "    ivtable = pd.concat([ivtable, new_row], ignore_index=True)\n",
    "ivtable.loc[ivtable['variable'] == 'DAYS EMPLOYED', 'IV'] = iv\n",
    "\n",
    "# Step 4: Convert gp_worktm to dummy variables\n",
    "new_data = convert_dummy(new_data, 'gp_worktm')\n",
    "\n",
    "# Step 5: Sort and display IV table\n",
    "ivtable = ivtable.sort_values(by='IV', ascending=False)\n",
    "print(ivtable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data['FAM MEMBERS'].value_counts(sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN or inf values in 'FAM MEMBERS' with a default value (e.g., 0) or handle them\n",
    "new_data['FAM MEMBERS'] = new_data['FAM MEMBERS'].fillna(0).replace([np.inf, -np.inf], 0).astype(int)\n",
    "\n",
    "# Create a new grouped column for 'FAM MEMBERS'\n",
    "new_data['FAM MEMBERS GP'] = new_data['FAM MEMBERS'].astype(object)\n",
    "\n",
    "# Group families with 3 or more members into a single category\n",
    "new_data.loc[new_data['FAM MEMBERS GP'] >= 3, 'FAM MEMBERS GP'] = '3more'\n",
    "\n",
    "# Calculate IV for 'FAM MEMBERS GP'\n",
    "iv, data = calc_iv(new_data, 'FAM MEMBERS GP', 'target')\n",
    "\n",
    "# Update IV table for 'FAM MEMBERS'\n",
    "ivtable.loc[ivtable['variable'] == 'FAM MEMBERS', 'IV'] = iv\n",
    "\n",
    "# Display the first few rows of the IV DataFrame\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = convert_dummy(new_data,'FAM MEMBERS GP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_data.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Income Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display value counts for 'INCOME TYPE' without sorting\n",
    "income_type_counts = new_data['INCOME TYPE'].value_counts(sort=False)\n",
    "print(\"Value counts (unsorted):\")\n",
    "print(income_type_counts)\n",
    "\n",
    "# Display normalized value counts for 'INCOME TYPE' without sorting\n",
    "income_type_normalized_counts = new_data['INCOME TYPE'].value_counts(normalize=True, sort=False)\n",
    "print(\"Normalized value counts (unsorted):\")\n",
    "print(income_type_normalized_counts)\n",
    "\n",
    "# Consolidate categories for 'INCOME TYPE'\n",
    "new_data['INCOME TYPE'] = new_data['INCOME TYPE'].replace({'Pensioner': 'State servant', 'Student': 'State servant'})\n",
    "\n",
    "# Calculate IV for 'INCOME TYPE'\n",
    "iv, data = calc_iv(new_data, 'INCOME TYPE', 'target')\n",
    "\n",
    "# Update the IV table\n",
    "ivtable.loc[ivtable['variable'] == 'INCOME TYPE', 'IV'] = iv\n",
    "\n",
    "# Display the first few rows of the resulting data\n",
    "print(\"First few rows of data after IV calculation:\")\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = convert_dummy(new_data,'INCOME TYPE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group occupations into broader categories\n",
    "new_data.loc[(new_data['TYPE'] == 'Cleaning staff') | \n",
    "             (new_data['TYPE'] == 'Cooking staff') | \n",
    "             (new_data['TYPE'] == 'Drivers') | \n",
    "             (new_data['TYPE'] == 'Laborers') | \n",
    "             (new_data['TYPE'] == 'Low-skill Laborers') | \n",
    "             (new_data['TYPE'] == 'Security staff') | \n",
    "             (new_data['TYPE'] == 'Waiters/barmen staff'), 'TYPE'] = 'Laborwk'\n",
    "\n",
    "new_data.loc[(new_data['TYPE'] == 'Accountants') | \n",
    "             (new_data['TYPE'] == 'Core staff') | \n",
    "             (new_data['TYPE'] == 'HR staff') | \n",
    "             (new_data['TYPE'] == 'Medicine staff') | \n",
    "             (new_data['TYPE'] == 'Private service staff') | \n",
    "             (new_data['TYPE'] == 'Realty agents') | \n",
    "             (new_data['TYPE'] == 'Sales staff') | \n",
    "             (new_data['TYPE'] == 'Secretaries'), 'TYPE'] = 'officewk'\n",
    "\n",
    "new_data.loc[(new_data['TYPE'] == 'Managers') | \n",
    "             (new_data['TYPE'] == 'High skill tech staff') | \n",
    "             (new_data['TYPE'] == 'IT staff'), 'TYPE'] = 'hightecwk'\n",
    "\n",
    "# Print the value counts for the TYPE column\n",
    "print(new_data['TYPE'].value_counts())\n",
    "\n",
    "# Calculate IV for 'TYPE'\n",
    "iv, data = calc_iv(new_data, 'TYPE', 'target')\n",
    "\n",
    "# Update the IV table for 'TYPE'\n",
    "ivtable.loc[ivtable['variable'] == 'TYPE', 'IV'] = iv\n",
    "\n",
    "# Display the first few rows of the IV DataFrame\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = convert_dummy(new_data,'TYPE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "House Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iv, data=calc_iv(new_data,'HOUSING TYPE','target')\n",
    "ivtable.loc[ivtable['variable']=='HOUSING TYPE','IV']=iv\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidate categories for 'EDUCATION TYPE'\n",
    "new_data.loc[new_data['EDUCATION TYPE'] == 'Academic degree', 'EDUCATION TYPE'] = 'Higher education'\n",
    "\n",
    "# Calculate IV for 'EDUCATION TYPE'\n",
    "iv, data = calc_iv(new_data, 'EDUCATION TYPE', 'target')\n",
    "\n",
    "# Update the IV table\n",
    "ivtable.loc[ivtable['variable'] == 'EDUCATION TYPE', 'IV'] = iv\n",
    "\n",
    "# Display the first few rows of the resulting data\n",
    "print(\"First few rows of data after IV calculation:\")\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = convert_dummy(new_data,'EDUCATION TYPE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Marriage Condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display normalized value counts for 'FAMILY STATUS' without sorting\n",
    "family_status_normalized_counts = new_data['FAMILY STATUS'].value_counts(normalize=True, sort=False)\n",
    "\n",
    "# Print the results\n",
    "print(\"Normalized value counts (unsorted):\")\n",
    "print(family_status_normalized_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iv, data=calc_iv(new_data,'FAMILY STATUS','target')\n",
    "ivtable.loc[ivtable['variable']=='FAMILY STATUS','IV']=iv\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = convert_dummy(new_data,'FAMILY STATUS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ivtable['IV'] = ivtable['IV'].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename variables in ivtable\n",
    "ivtable.loc[ivtable['variable'] == 'DAYS SINCE BIRTH', 'IV'] = iv\n",
    "ivtable.loc[ivtable['variable'] == 'DAYS EMPLOYED', 'variable'] = 'DAYS EMPLOYEED'\n",
    "ivtable.loc[ivtable['variable'] == 'inc', 'variable'] = 'incgp'\n",
    "\n",
    "# Sort ivtable by IV\n",
    "ivtable = ivtable.sort_values(by='IV', ascending=False)\n",
    "\n",
    "# Verify ivtable\n",
    "print(ivtable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "# selected_columns = ['GENDER', 'OWN CAR', 'OWN REALTY', 'INCOME TOTAL',\n",
    "#          'FAM MEMBERS', 'CHILDREN_1', 'CHILDREN_2More', 'gp_inc_medium', 'gp_inc_high',\n",
    "#        'gp_inc_NULL', 'FAM MEMBERS GP_1',\n",
    "#        'FAM MEMBERS GP_3more', 'INCOME TYPE_Commercial associate',\n",
    "#         'INCOME TYPE_State servant',\n",
    "#         'TYPE_Laborwk', 'TYPE_hightecwk',\n",
    "#        'TYPE_officewk', \n",
    "#        'EDUCATION TYPE_Higher education', 'EDUCATION TYPE_Incomplete higher',\n",
    "#        'EDUCATION TYPE_Lower secondary','DAYS SINCE BIRTH','FAMILY STATUS_Civil marriage','FAMILY STATUS_Separated' ,'FAMILY STATUS_Single / not married',\n",
    "#                     'FAMILY STATUS_Widow' ,'DAYS EMPLOYED','Age']\n",
    "\n",
    "# # Data Preparation\n",
    "# # Assuming new_data is your DataFrame\n",
    "# # Ensure target variable is an integer\n",
    "# Y = new_data['target'].astype('int')\n",
    "# X = new_data[selected_columns]\n",
    "Y = new_data['target'].astype('int')\n",
    "\n",
    "# Define features (X) - Drop '_id' and 'ID' as they are non-numeric and not meaningful\n",
    "X = new_data.drop(columns=['_id', 'ID','target','CHILDREN_1', 'CHILDREN_2More','DAYS SINCE BIRTH', 'DAYS EMPLOYED'])\n",
    "\n",
    "# Check and ensure all columns in X are numeric\n",
    "numeric_columns = X.select_dtypes(include=['number']).columns\n",
    "X = X[numeric_columns]\n",
    "\n",
    "# Apply SMOTE to balance the dataset\n",
    "sm = SMOTE(random_state=42)\n",
    "X_balance, Y_balance = sm.fit_resample(X, Y)\n",
    "\n",
    "# Convert X_balance back to a DataFrame\n",
    "X_balance = pd.DataFrame(X_balance, columns=X.columns)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_balance, Y_balance, \n",
    "                                                    stratify=Y_balance, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=10086)\n",
    "\n",
    "# Train a Logistic Regression Model\n",
    "model = LogisticRegression(C=0.8, random_state=0, solver='lbfgs', max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_predict = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print('Accuracy Score is {:.5}'.format(accuracy_score(y_test, y_predict)))\n",
    "print(\"Confusion Matrix:\\n\", pd.DataFrame(confusion_matrix(y_test, y_predict)))\n",
    "\n",
    "# Plot the Confusion Matrix\n",
    "sns.set_style('white') \n",
    "\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion Matrix', cmap='coolwarm'):\n",
    "    \"\"\"\n",
    "    Plots the confusion matrix.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt=\".2f\" if normalize else \"d\", cmap=cmap,\n",
    "                xticklabels=classes, yticklabels=classes)\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "\n",
    "cm = confusion_matrix(y_test, y_predict)\n",
    "class_names = ['0', '1']  # Replace with your actual class labels if necessary\n",
    "\n",
    "plot_confusion_matrix(cm, class_names, normalize=True, \n",
    "                      title='Normalized Confusion Matrix: Logistic Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"iframe_connected\"  \n",
    "selected_columns = ['GENDER', 'OWN CAR', 'OWN REALTY', 'INCOME TOTAL',\n",
    "         'FAM MEMBERS', 'CHILDREN_1', 'CHILDREN_2More', 'gp_inc_medium', 'gp_inc_high',\n",
    "       'gp_inc_NULL', 'FAM MEMBERS GP_1',\n",
    "       'FAM MEMBERS GP_3more', 'INCOME TYPE_Commercial associate',\n",
    "        'INCOME TYPE_State servant',\n",
    "        'TYPE_Laborwk', 'TYPE_hightecwk',\n",
    "       'TYPE_officewk', \n",
    "       'EDUCATION TYPE_Higher education', 'EDUCATION TYPE_Incomplete higher',\n",
    "       'EDUCATION TYPE_Lower secondary','DAYS SINCE BIRTH','FAMILY STATUS_Civil marriage','FAMILY STATUS_Separated' ,'FAMILY STATUS_Single / not married',\n",
    "                    'FAMILY STATUS_Widow' ,'DAYS EMPLOYED','Age']\n",
    "X = new_data[selected_columns].fillna(0)  \n",
    "Y = new_data['target']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "pca_df = pd.DataFrame(X_pca, columns=['PCA1', 'PCA2', 'PCA3'])\n",
    "pca_df['target'] = Y\n",
    "\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "print(f\"Explained Variance Ratio: {explained_variance}\")\n",
    "print(f\"Total Explained Variance: {np.sum(explained_variance)}\")\n",
    "\n",
    "fig = px.scatter_3d(\n",
    "    pca_df,\n",
    "    x='PCA1',\n",
    "    y='PCA2',\n",
    "    z='PCA3',\n",
    "    color='target',\n",
    "    title=\"PCA 3D Visualization\"\n",
    ")\n",
    "fig.update_traces(marker=dict(opacity=0.5))  \n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Neural Networks with only values high on the IV chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually specify the selected columns\n",
    "selected_columns = [\n",
    "    'GENDER',\n",
    "    'OWN REALTY',\n",
    "    'gp_Age_low',\n",
    "    'gp_Age_medium',\n",
    "    'gp_Age_high',\n",
    "    'gp_Age_highest',\n",
    "    'gp_worktm_high',\n",
    "    'gp_worktm_highest',\n",
    "    'gp_worktm_low',\n",
    "    'gp_worktm_medium',\n",
    "    'FAMILY STATUS_Civil marriage',\n",
    "    'FAMILY STATUS_Separated',\n",
    "    'FAMILY STATUS_Single / not married',\n",
    "    'FAMILY STATUS_Widow'\n",
    "]\n",
    "\n",
    "# we select the high iv columns\n",
    "Y = new_data['target']\n",
    "X = new_data[selected_columns]\n",
    "\n",
    "Y = Y.astype('int')\n",
    "X_balance,Y_balance = SMOTE().fit_resample(X,Y)\n",
    "X_balance = pd.DataFrame(X_balance, columns = X.columns)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_balance,Y_balance, \n",
    "                                                    stratify=Y_balance, test_size=0.3,\n",
    "                                                    random_state = 10086)\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "# Create a StandardScaler instances\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.\n",
    "number_input_features = X_train_scaled.shape[1]  # Correct the number of features\n",
    "hidden_nodes_layer1 = 80\n",
    "hidden_nodes_layer2 = 30\n",
    "hidden_nodes_layer3 = 10\n",
    "\n",
    "nn = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation='relu'))\n",
    "\n",
    "# Second hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation='relu'))\n",
    "\n",
    "# Third hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer3, activation='sigmoid'))\n",
    "\n",
    "# Output layer\n",
    "nn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Check the structure of the model\n",
    "nn.summary()\n",
    "\n",
    "# Compile the model\n",
    "nn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "fit_model = nn.fit(X_train_scaled, y_train, epochs=15)\n",
    "\n",
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict probabilities\n",
    "y_pred_prob = nn.predict(X_test_scaled)\n",
    "\n",
    "# Convert probabilities to class predictions (e.g., threshold of 0.5)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance Interpretation for High-Risk vs. No-Risk\n",
    "Current Accuracy (65.31%):\n",
    "Suggests that ~35% of samples are misclassified, but it doesn't reveal if high-risk cases are disproportionately affected.\n",
    "If high-risk cases are a minority, the model may overfit to predicting the majority (no-risk), inflating accuracy but underperforming where it matters.\n",
    "Loss (0.6021):\n",
    "Indicates moderate uncertainty in predictions; may suggest overlap between high- and no-risk feature distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All columns except target and objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all columns with object data type\n",
    "object_columns = new_data.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Print the list of object columns\n",
    "print(\"Object columns in the DataFrame:\")\n",
    "print(object_columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all columns excluding object data types and the 'target' column\n",
    "selected_columns = new_data.select_dtypes(exclude=['object']).drop(columns=['target']).columns.tolist()\n",
    "\n",
    "# View the resulting list of columns\n",
    "print(selected_columns)\n",
    "\n",
    "# we select the high iv columns\n",
    "Y = new_data['target']\n",
    "X = new_data[selected_columns]\n",
    "\n",
    "Y = Y.astype('int')\n",
    "X_balance,Y_balance = SMOTE().fit_resample(X,Y)\n",
    "X_balance = pd.DataFrame(X_balance, columns = X.columns)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_balance,Y_balance, \n",
    "                                                    stratify=Y_balance, test_size=0.3,\n",
    "                                                    random_state = 10086)\n",
    "\n",
    "# Create a StandardScaler instances\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.\n",
    "number_input_features = X_train_scaled.shape[1]  # Correct the number of features\n",
    "hidden_nodes_layer1 = 80\n",
    "hidden_nodes_layer2 = 30\n",
    "hidden_nodes_layer3 = 10\n",
    "\n",
    "nn = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation='relu'))\n",
    "\n",
    "# Second hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation='relu'))\n",
    "\n",
    "# Third hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer3, activation='sigmoid'))\n",
    "\n",
    "# Output layer\n",
    "nn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Check the structure of the model\n",
    "nn.summary()\n",
    "\n",
    "# Compile the model\n",
    "nn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "fit_model = nn.fit(X_train_scaled, y_train, epochs=15)\n",
    "\n",
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict probabilities\n",
    "y_pred_prob = nn.predict(X_test_scaled)\n",
    "\n",
    "# Convert probabilities to class predictions (e.g., threshold of 0.5)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why Such a Large Improvement?\n",
    "The improvement can be attributed to the inclusion of more features, providing the model with richer information to make decisions:\n",
    "\n",
    "Feature Completeness:\n",
    "By excluding only object columns and the target, the model now has access to more relevant numerical and categorical data (potentially after encoding), which contributes to its ability to capture patterns in the data.\n",
    "Complex Relationships:\n",
    "More features allow the model to learn complex interactions between variables that were missing when only 5 high-IV features were used.\n",
    "Diminished Feature Selection Bias:\n",
    "Relying only on high-IV features may exclude some important interactions or complementary features. Including all numeric columns mitigates this bias.\n",
    "\n",
    "Accuracy: 97.72%\n",
    "What is it?\n",
    "Accuracy measures the proportion of correct predictions (both high-risk and no-risk) relative to the total predictions.\n",
    "Interpretation:\n",
    "An accuracy of 97.72% indicates that the model predicts the correct class for most of the samples.\n",
    "This is a substantial improvement over the previous accuracy of 65.31%, highlighting that the additional features provide more comprehensive information for the model to learn from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
