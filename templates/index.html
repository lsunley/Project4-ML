<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <title>Project4</title>

  <!-- Bootstrap CSS -->
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css" rel="stylesheet"
    integrity="sha384-T3c6CoIi6uLrA9TneNEoa7RxnatzjcDSCmG1MXxSR1GAsXEV/Dwwykc2MPK8M2HN" crossorigin="anonymous">

  <!-- Custom CSS -->
  <link rel="stylesheet" type="text/css" href="{{ url_for('static', filename='css/style.css') }}">
</head>

<body>
  <div class="container">


    <div class="row">
      <!-- Image Section -->
      <div class="col-md-12 text-center my-4">
        <img src="{{ url_for('static', filename='../iframe_figures/card.jpg') }}" alt="Card Image" class="img-fluid"
          style="width: 100%">
      </div>
    </div>

    <div class="row">

      <!-- IFrame Section -->
      <div class="col-md-12 text-center">
        <iframe src="{{ url_for('static', filename='../iframe_figures/figure_63.html') }}" width="800" height="600"
          style="border:none;"></iframe>
      </div>


      <div class=" p-4 border rounded shadow-sm bg-light">
        <h3 class="text-center text-primary mb-4">Summary</h3>
        <p class="text-justify">
          After comparison, it is found that the best feature is the following feature in <strong>try2</strong>, and the
          accuracy on the test set is <strong>0.9588</strong>.
        </p>
        <h4 class="text-secondary">Classifier Performance:</h4>
        <ul class="list-group mb-3">
          <li>
            <strong>Class 0 (No Risk):</strong> The classifier achieved an accuracy of <strong>97% (0.97)</strong>,
            meaning most samples with a true label of 0 (no risk) were correctly classified.
          </li>
          <li>
            <strong>Class 1 (Have Risk):</strong> The classifier achieved a higher accuracy of <strong>94%
              (0.94)</strong>, indicating stronger performance in identifying class 1 (have risk).
          </li>
        </ul>
        <p>
          The model demonstrates good overall performance, but the relatively high misclassification rate for class 0
          indicates room for improvement.
        </p>
      </div>
      <div class="row mt-5">
        <h3 class="text-center">Confusion Matrix</h3>
        <div class="col-md-12 text-center">
          <img src="{{ url_for('static', filename='../iframe_figures/predict.jpg') }}" alt="Card Image"
            class="img-fluid">
        </div>
      </div>
      <div class="row flex-container1 mt-4">
        <div class="col-md-4 elbow">
          <h4>Logistic Regression:</h4>
          <p>
            The model shows a fair balance in predictions but has moderate misclassifications for both classes.
          </p>
        </div>
        <div class="col-md-4 elbow">
          <h4>XGBoost:</h4>
          <p>
            Provides the best performance with minimal misclassification and near-perfect balance for both classes.
          </p>
        </div>
        <div class="col-md-4 elbow">
          <h4>Voting Classifier:</h4>
          <p>
            Combines Logistic Regression and XGBoost, achieving high accuracy close to XGBoost.
          </p>
        </div>
      </div>
      <div class="mt-5 p-4 border rounded shadow-sm bg-light">
        <h3 class="text-center text-primary mb-4">Summary</h3>
        <ul class="list-group mb-3">
          <li>Logistic Regression assumes a linear relationship between the features and the target class. If the data
            is complex and non-linear (as is often the case in real-world problems), it may underperform.</li>
          <li>XGBoost is a tree-based model capable of learning non-linear relationships and complex patterns in the
            data.</li>
          <li>While XGBoost dominates in performance, Logistic Regression can help stabilize predictions in cases where
            XGBoost may overfit or make errors. The Voting Classifier takes advantage of their complementary strengths.
          </li>
        </ul>
      </div>
      <div class="row flex-container mt-4">
        <!-- Image Section -->
        <h3 class="text-center">Neural Networks with only values high on the IV chart</h3>
        <div class="col-md-6 text-center">
          <img src="{{ url_for('static', filename='../iframe_figures/NN_IV_results.jpg') }}" alt="Card Image"
            class="img-fluid">
        </div>
        <div class="col-md-6 text-center">
          <img src="{{ url_for('static', filename='../iframe_figures/NN_ALL_results.jpg') }}" alt="Card Image"
            class="img-fluid">

        </div>

        <div class="row flex-container mt-4">
          <!-- Image Section -->
          <div class="col-md-6 text-center">
            <img src="{{ url_for('static', filename='../iframe_figures/NN_IV.jpg') }}" alt="Card Image"
              class="img-fluid">
          </div>
          <div class="col-md-6 text-center">
            <img src="{{ url_for('static', filename='../iframe_figures/NN_ALL.jpg') }}" alt="Card Image"
              class="img-fluid">
          </div>

        </div>
      </div>


      <div class="mt-5 p-4 border rounded shadow-sm bg-light">
        <h3 class="text-center text-primary mb-4">Summary</h3>
        <ul class="list-group mb-3">
          <li>
            <strong>Feature Completeness:</strong>
            By excluding only object columns and the target, the model now has access to more relevant numerical and
            categorical data (potentially after encoding), which contributes to its ability to capture patterns in the
            data.
          </li>
          <li>
            <strong>Complex Relationships:</strong>
            More features allow the model to learn complex interactions between variables that were missing when only 5
            high-IV features were used.
          </li>
          <li>
            <strong>Diminished Feature Selection Bias:</strong>
            Relying only on high-IV features may exclude some important interactions or complementary features.
            Including all numeric columns mitigates this bias.
          </li>


        </ul>
      </div>
      
      <div class="row mt-5">
        <h3 class="text-center">Elbow Curve Analysis for Optimal K</h3>
        <div class="col-md-12 text-center">
          <img src="{{ url_for('static', filename='../iframe_figures/Elbow.jpg') }}" alt="Card Image" class="img-fluid">
        </div>
      </div>

      <!-- Flex Container for Elbow Explanation -->
      <div class="row flex-container1 mt-4">
        <div class="col-md-6 elbow">
          <h4>Elbow Curve (Credits Data):</h4>
          <p>
            There is a flattening point (elbow) observed around k = 5 or 6, suggesting this is the optimal number of
            clusters.
            Beyond this point, the decrease in inertia becomes less significant. Therefore, the best K is 5 at this
            point.
          </p>
        </div>
        <div class="col-md-6 elbow ml-2">
          <h4>Elbow Curve (PCA-transformed Data):</h4>
          <p>
            Similar to the left plot, this analysis is performed on PCA-reduced data. The curve also flattens around k =
            5 or 6,
            reinforcing that 5 or 6 clusters might be the best choice for the PCA-reduced dataset.
          </p>
        </div>
      </div>

      <!-- Final Summary -->
      <div class="mt-5 p-4 border rounded shadow-sm bg-light">
        <h3 class="text-center text-primary mb-4">Summary</h3>
        <ul class="list-group mb-3">
          <li>The optimal number of clusters (k) for both raw and PCA-transformed data is likely 5 or 6.</li>
          <li>PCA simplifies the feature space, improving computational efficiency and possibly cluster separation.</li>
        </ul>
      </div>

      <div class="row mt-5">
        <div class="col-md-12 text-center">
          <img src="{{ url_for('static', filename='../iframe_figures/Cluster.jpg') }}" alt="Card Image"
            class="img-fluid">
        </div>
      </div>
      <div class="row flex-container1 mt-4">
        <div class="col-md-6 elbow">
          <h4>Cluster Crypto:</h4>
          <p>
            The clusters overlap in several areas, but there are some distinct groupings, particularly in regions with
            higher income and fewer days since birth.
          </p>
        </div>
        <div class="col-md-6 elbow ml-2">
          <h4>Cluster PCA:</h4>
          <p>
            Clusters in the PCA space are more distinct and better separated compared to the raw data plot, indicating
            improved clustering results after dimensionality reduction.
          </p>
        </div>
      </div>
      <div class="mt-5 p-4 border rounded shadow-sm bg-light">
        <h3 class="text-center text-primary mb-4">Summary</h3>

        <ul class="list-group mb-3">
          <li>
            The PCA-transformed space (right plot) demonstrates better separation of clusters compared to the raw
            feature space (left plot), likely due to reduced noise and improved representation of variance.
          </li>

        </ul>

      </div>
    </div>

    <!-- External JavaScript Libraries -->
    <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
    <script src="https://d3js.org/d3.v7.min.js"></script>

    <!-- Custom JavaScript -->
    <script src="{{ url_for('static', filename='script.js') }}"></script>
</body>

</html>